{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97d7d21a-026b-49c4-b029-f40bdca22b50",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "#### Random Forest is an ensemble learning algorithm that:\n",
    "#### Combines multiple decision trees using bagging + random feature selection.\n",
    "\n",
    "#### It is used for:\n",
    "<ul>\n",
    "    <li>Classification</li>\n",
    "    <li>Regression</li>\n",
    "</ul>\n",
    "\n",
    "---\n",
    "\n",
    "## How Random Forest Works\n",
    "#### Random Forest builds many decision trees using:\n",
    "#### Step 1: Bootstrap Sampling (Bagging)\n",
    "<ul>\n",
    "    <li>Random subset of training data (with replacement)</li>\n",
    "</ul>\n",
    "\n",
    "#### Step 2: Random Feature Selection\n",
    "<ul>\n",
    "    <li>At each split, only a random subset of features is considered</li>\n",
    "</ul>\n",
    "\n",
    "#### Step 3: Aggregation\n",
    "<ul>\n",
    "    <li>Classification → Majority voting</li>\n",
    "    <li>Regression → Averaging</li>\n",
    "</ul>\n",
    "\n",
    "---\n",
    "\n",
    "## Differences between bagging and random forest\n",
    "<ul>\n",
    "    <li>Random forest uses only decision tree as base model whereas bagging can use any classifier.</li>\n",
    "    <li> Node level feature(column) sampling in random forest where as Tree level feature sampling in bagging.</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Hyperparameter Tuning in Random Forest using GridSearchCV and RandomizedSearchCV\n",
    "\n",
    "### Why Hyperparameter Tuning is Needed?\n",
    "#### Random Forest has many hyperparameters like:\n",
    "<ul>\n",
    "    <li>n_estimators</li>\n",
    "    <li>max_depth</li>\n",
    "    <li>max_features</li>\n",
    "    <li>min_samples_split</li>\n",
    "    <li>min_samples_leaf</li>\n",
    "</ul>\n",
    "\n",
    "#### Default values work well, but:\n",
    "#### Proper tuning improves performance and reduces overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "## Hyperparameter Tuning with GridSearchCV\n",
    "#### Tries ALL possible combinations of given hyperparameters.\n",
    "#### It performs:\n",
    "<ul>\n",
    "    <li>Cross-validation</li>\n",
    "    <li>For every combination</li>\n",
    "    <li>Selects best one</li>\n",
    "</ul>\n",
    "\n",
    "### How It Works\n",
    "#### If:\n",
    "```\n",
    "n_estimators = [100, 200]\n",
    "max_depth = [5, 10]\n",
    "```\n",
    "#### Total combinations:\n",
    "``` 2 × 2 = 4 models ```\n",
    "#### Each model evaluated with cross-validation.\n",
    "\n",
    "### Code Example (GridSearchCV)\n",
    "```\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],  # no. of trees in forest\n",
    "    'max_depth': [None, 5, 10],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'min_samples_split': [2, 5]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,  # 5-fold cross validation\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", grid.best_params_)\n",
    "print(\"Best Score:\", grid.best_score_)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## RandomizedSearchCV\n",
    "#### Instead of trying ALL combinations:\n",
    "#### Randomly samples a fixed number of parameter combinations.\n",
    "\n",
    "### Why Use It?\n",
    "#### When:\n",
    "<ul>\n",
    "    <li>Many hyperparameters</li>\n",
    "    <li>Large dataset</li>\n",
    "    <li>Large parameter ranges</li>\n",
    "</ul>\n",
    "\n",
    "### Code Example (RandomizedSearchCV)\n",
    "```\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.stats import randint\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators': randint(100, 500),  \n",
    "    # Number of trees in the forest (random integer between 100 and 500)\n",
    "    \n",
    "    'max_depth': randint(3, 20),  # controls overfitting\n",
    "    \n",
    "    'min_samples_split': randint(2, 10),  \n",
    "    # Minimum samples required to split a node\n",
    "    \n",
    "    'min_samples_leaf': randint(1, 5),  \n",
    "    # Minimum samples required at a leaf node\n",
    "    \n",
    "    'max_features': ['sqrt', 'log2', None]  \n",
    "    # Number of features considered at each split\n",
    "    # 'sqrt' = sqrt(total features)\n",
    "    # 'log2' = log2(total features)\n",
    "    # None = use all features\n",
    "}\n",
    "\n",
    "# Create RandomizedSearchCV object\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=rf,  \n",
    "    param_distributions=param_dist,  \n",
    "    n_iter=20,   # Number of random parameter combinations to try\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    scoring='accuracy',  # Metric used to evaluate performance\n",
    "    random_state=42,   # Reproducibility\n",
    "     n_jobs=-1   # Use all CPU cores for faster computation\n",
    ")\n",
    "\n",
    "# Perform hyperparameter tuning\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Print best parameter combination found\n",
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "\n",
    "# Print best cross-validation accuracy score\n",
    "print(\"Best Score:\", random_search.best_score_)\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## OOB (Out-Of-Bag)\n",
    "#### OOB score is an internal validation method used in:\n",
    "<ul>\n",
    "    <li>Bagging</li>\n",
    "    <li>Random Forest</li>\n",
    "    <li>Extra Trees</li>\n",
    "</ul>\n",
    "\n",
    "#### It estimates: Model performance on unseen data without using a separate validation set.\n",
    "#### How to Use OOB in sklearn\n",
    "```\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    bootstrap=True,\n",
    "    oob_score=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "print(\"OOB Score:\", rf.oob_score_)\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
