{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "752014f5-7f02-43a2-9acd-8586f836d56b",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA)\n",
    "#### PCA is a dimensionality reduction technique used to reduce the number of features while preserving as much information (variance) as possible.\n",
    "\n",
    "### In simple words:\n",
    "#### PCA converts many correlated features into fewer uncorrelated new features.\n",
    "\n",
    "## Why Do We Need PCA?\n",
    "### When:\n",
    "<ul>\n",
    "    <li>Dataset has many features</li>\n",
    "    <li>Features are highly correlated</li>\n",
    "    <li>Curse of dimensionality occurs</li>\n",
    "    <li>Computation becomes expensive</li>\n",
    "    <li>Overfitting risk increases</li>\n",
    "</ul>\n",
    "\n",
    "#### PCA helps by reducing dimensions.\n",
    "\n",
    "---\n",
    "\n",
    "## Core Idea of PCA\n",
    "#### Instead of using original features:\n",
    "```sh\n",
    "X1, X2, X3, X4\n",
    "```\n",
    "\n",
    "#### PCA creates:\n",
    "```sh\n",
    "PC1, PC2, PC3, PC4\n",
    "```\n",
    "#### Where:\n",
    "<ul>\n",
    "    <li>PC1 → captures maximum variance</li>\n",
    "    <li>PC2 → captures second highest variance</li>\n",
    "    <li>PC3 → next</li>\n",
    "    <li>All PCs are orthogonal (uncorrelated)</li>\n",
    "</ul>\n",
    "\n",
    "---\n",
    "\n",
    "## Mathematical Idea (Conceptual)\n",
    "<ol>\n",
    "    <li>Standardize data</li>\n",
    "    <li>Compute covariance matrix</li>\n",
    "    <li>Compute eigenvalues & eigenvectors</li>\n",
    "    <li>Choose top eigenvectors</li>\n",
    "    <li>Project data onto them</li>\n",
    "</ol>\n",
    "\n",
    "---\n",
    "\n",
    "## Important Terms\n",
    "## 1️⃣ Principal Components (PCs)\n",
    "#### New transformed features.\n",
    "\n",
    "## 2️⃣ Eigenvectors\n",
    "#### Directions of maximum variance.\n",
    "\n",
    "## 3️⃣ Eigenvalues\n",
    "#### Amount of variance captured by each component.\n",
    "\n",
    "---\n",
    "\n",
    "## Explained Variance\n",
    "#### Each component captures some percentage of total variance.\n",
    "### Example:\n",
    "```sh\n",
    "Component\tExplained Variance\n",
    "    PC1\t           60%\n",
    "    PC2\t           25%\n",
    "    PC3\t           10%\n",
    "    PC4\t            5% \n",
    "```\n",
    "#### You might keep only PC1 & PC2 (85% variance retained).\n",
    "\n",
    "---\n",
    "\n",
    "## In Python\n",
    "```sh\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# 1 Scale data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 2 Apply PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "print(pca.explained_variance_ratio_)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Choosing Number of Components\n",
    "### Method 1: Explained Variance\n",
    "```sh\n",
    "pca = PCA(n_components=0.95)\n",
    "```\n",
    "#### Keeps enough components to retain 95% variance.\n",
    "\n",
    "### Method 2: Scree Plot\n",
    "#### Plot cumulative explained variance and find elbow point.\n",
    "\n",
    "---\n",
    "\n",
    "## When to Use PCA?\n",
    "<ul>\n",
    "    <li>When features are correlated</li>\n",
    "    <li>When dimensionality is high</li>\n",
    "    <li>Before KNN or clustering</li>\n",
    "    <li>For visualization (2D/3D)</li>\n",
    "</ul>\n",
    "\n",
    "## Important Points\n",
    "<ul>\n",
    "    <li>PCA is unsupervised (does not use target variable)</li>\n",
    "    <li>PCA creates linear combinations of features</li>\n",
    "    <li>PCA reduces interpretability</li>\n",
    "    <li>Must scale data before applying PCA</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a535aba2-dfe1-4b68-8a7b-023a690efcaf",
   "metadata": {},
   "source": [
    "## What PCA Actually Does Geometrically\n",
    "### Variance = Spread\n",
    "#### 1. Rotates coordinate system\n",
    "#### 2. Projects data onto these new axes\n",
    "#### 3. If most variance is along PC1: We can drop PC2 with little information loss.\n",
    "#### That’s dimensionality reduction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
